{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "16ef3e4f85a17b9d756739c763555fcc5591ee07"
      },
      "cell_type": "code",
      "source": "reviews_train = []\nfor line in open('../input/movie_data/movie_data/full_train.txt', 'r'):\n    \n    reviews_train.append(line.strip())\n    \nreviews_test = []\nfor line in open('../input/movie_data/movie_data/full_test.txt', 'r'):\n    \n    reviews_test.append(line.strip())\n    \ntarget = [1 if i < 12500 else 0 for i in range(25000)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75d321e34c7919e7c6b330fc6fc2193dae34056e"
      },
      "cell_type": "code",
      "source": "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\nREPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\nNO_SPACE = \"\"\nSPACE = \" \"\n\ndef preprocess_reviews(reviews):\n    \n    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n    \n    return reviews\n\nreviews_train_clean = preprocess_reviews(reviews_train)\nreviews_test_clean = preprocess_reviews(reviews_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a8e392d4fc9a5d126813b408d5725f7b3c123fb2"
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\n\nenglish_stop_words = stopwords.words('english')\n\ndef remove_stop_words(corpus):\n    removed_stop_words = []\n    for review in corpus:\n        removed_stop_words.append(\n            ' '.join([word for word in review.split() \n                      if word not in english_stop_words])\n        )\n    return removed_stop_words",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f0b9ddc020bcfe7861b2ea8e0041a1845ef3d12"
      },
      "cell_type": "code",
      "source": "no_stop_words_train = remove_stop_words(reviews_train_clean)\nno_stop_words_test = remove_stop_words(reviews_test_clean)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4ed1b8cb135d9e74e0cf6d710a7022a2a9a7b64"
      },
      "cell_type": "code",
      "source": "all_text = ' '.join([text for text in no_stop_words_train])\nprint('Number of words in all_text:', len(all_text))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e67148cb61e7428d2fb1903a8b5472b4f2596c57"
      },
      "cell_type": "code",
      "source": "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_text)\nplt.figure(figsize=(15, 12))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a94dc0820953bf562e5b5eccf9afd4dd7f965fc1"
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\n\nvocab_size = 5000\nmax_words = 500\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(no_stop_words_train)\n\nX_train = tokenizer.texts_to_sequences(no_stop_words_train)\nX_test = tokenizer.texts_to_sequences(no_stop_words_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "76a734138d76a8ba247de115cc2113b894cf86b9"
      },
      "cell_type": "code",
      "source": "#stopword_vectorizer = CountVectorizer()\n#stopword_vectorizer.fit(no_stop_words_train)\n#print(\"Size of dictionary: \", len(stopword_vectorizer.get_feature_names()))\n#print(\"Words in dictionary: \", stopword_vectorizer.get_feature_names())\n\n#X_train = stopword_vectorizer.transform(no_stop_words_train).toarray()\n#X_test = stopword_vectorizer.transform(no_stop_words_test).toarray()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60969bb2b7e3123e2a7cefbb87cf49fd9ca5187c"
      },
      "cell_type": "code",
      "source": "from keras.preprocessing import sequence\nX_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_test = sequence.pad_sequences(X_test, maxlen=max_words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba40a096e9038554f7e737a1d17fb9ecfb40226c"
      },
      "cell_type": "code",
      "source": "X_train, X_val, y_train, y_val = train_test_split(X_train, target, train_size = 0.9)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b779cc10f7d36c2ccf3226ccfeed51944e44799"
      },
      "cell_type": "code",
      "source": "X_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa27426d90d77c2b17a9d00ca9f5d1f5b8d2da01"
      },
      "cell_type": "code",
      "source": "from keras.models import Sequential\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, MaxPool1D\nfrom keras.preprocessing import sequence\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.constraints import max_norm\n\nreduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=0.000001, verbose=1)\nearly_stopper = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=1, mode='auto')\n\nembedding_size = 32",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "58782be0ed67089c1a07abcca1adb7ac48c51c3c"
      },
      "cell_type": "code",
      "source": "'''batch_size = [64, 128, 256]\nepochs = [10, 20, 30]\noptimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\nlearn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\nmomentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\nneurons = [50, 100, 150, 200]\nweight_constraint = [1, 2, 3, 4, 5]\ndropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nparam_grid = dict(batch_size=batch_size, epochs=epochs, optimizer=optimizer, learn_rate=learn_rate, \n                  dropout_rate=dropout_rate, weight_constraint=weight_constraint, momentum=momentum)'''\n\nneurons = [50, 100, 200]\ndropout = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\noptimizers = ['rmsprop', 'adam', 'sgd']\nbatch_size = [64, 128, 256]\nepochs = [15, 30]\nparam_grid = dict(neurons=neurons, optimizer=optimizers, dropout_rate=dropout, batch_size=batch_size, epochs=epochs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af690a325c7bb7ea9a650d3fa45f1f928d351228"
      },
      "cell_type": "code",
      "source": "'''def build_model(architecture='lstm'):\n    if architecture == 'embedding':\n        model = Sequential()\n        #model.add(Embedding(len(stopword_vectorizer.get_feature_names()), 32, input_length=max_words))\n        model.add(Embedding(vocab_size, 32, input_length=max_words))\n        model.add(Flatten())\n        model.add(Dense(500, activation='relu'))\n        model.add(Dense(1, activation='sigmoid'))\n    elif architecture == 'cnn':\n        model=Sequential()\n        model.add(Embedding(vocab_size, embedding_size, input_length=max_words))\n        model.add(Dropout(0.2))\n        model.add(Conv1D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n        model.add(MaxPool1D(pool_size = 2))\n        model.add(LSTM(100))\n        model.add(Dropout(0.2))\n        model.add(Dense(1, activation = 'sigmoid'))\n    elif architecture == 'lstm':\n        model = Sequential()\n        model.add(Embedding(vocab_size, embedding_size, input_length=max_words))\n        model.add(LSTM(neurons, activation = 'tanh', recurrent_activation='hard_sigmoid', dropout=dropout_rate))\n        model.add(Dense(1, activation='sigmoid'))\n    else:\n        raise NameError(\"Model name not found.\")\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model'''",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e33cdbbebc54cada7f09c185fa78205ba5840f3"
      },
      "cell_type": "code",
      "source": "def build_model(neurons, optimizer, dropout_rate):\n    model = Sequential()\n    model.add(Embedding(vocab_size, embedding_size, input_length=max_words))\n    model.add(LSTM(neurons, activation='tanh', recurrent_activation='hard_sigmoid', dropout=dropout_rate))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e22c77005cf76791c85c7bfd0f45fa4ddcf3aaf"
      },
      "cell_type": "code",
      "source": "model = KerasClassifier(build_fn=build_model, verbose=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "537c3fd1df9e18a17eeb1bf3762f14d69d1c0e73"
      },
      "cell_type": "code",
      "source": "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, verbose=1)\n#grid_result = grid.fit(X_train, y_train)\ngrid_result = grid.fit(X_train, y_train, callbacks=[reduce_lr, early_stopper])\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39e03bbd9d8f1f7b5445602cf5104320db5dd3eb"
      },
      "cell_type": "code",
      "source": "'''history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, batch_size=128, verbose=1, callbacks=[reduce_lr, earlt_stopper]).history\n\nplt.plot(history['acc'], linewidth=2, label='Train')\nplt.plot(history['val_acc'], linewidth=2, label='Test')\nplt.legend(loc='upper right')\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()\n\nplt.plot(history['loss'], linewidth=2, label='Train')\nplt.plot(history['val_loss'], linewidth=2, label='Test')\nplt.legend(loc='upper right')\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.show()'''",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cae0191c9cf526158c92a12e297db980643a1a18"
      },
      "cell_type": "code",
      "source": "!pip install talos",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3cdb5a5f50f71c5323e780d5c8bc5f30aeffb34d"
      },
      "cell_type": "code",
      "source": "import talos as ta",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af39ff0bce693a9fe91f72187630711217dd1385"
      },
      "cell_type": "code",
      "source": "def imdb_fn(x_train, y_train, x_val, y_val, params):\n    \n    dropout = float(params['dropout'])\n    lstm_neuron = int(params['lstm_neuron'])\n    \n    model = Sequential()\n    model.add(Embedding(vocab_size, embedding_size, input_length=max_words))\n    model.add(LSTM(lstm_neuron, activation='tanh', recurrent_activation='hard_sigmoid', dropout=dropout))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(\n        optimizer=params['optimizer'],\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n\n\n    out = model.fit(\n        x_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], \n        verbose=1,\n        validation_data=[x_val, y_val]\n    )\n    \n    return out, model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b157a26f23c662b35fbef8bf083f1abab86d8ff9"
      },
      "cell_type": "code",
      "source": "np.array(y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7406ebf80b97f8b2754e3d3ec81259f33413de90"
      },
      "cell_type": "code",
      "source": "para = {\n    'epochs': [10, 15, 20],\n    'batch_size': [32, 64, 128],\n    'lstm_neuron': [100, 200, 300],\n    'optimizer': ['adam', 'rmsprop', 'sgd'],\n    'dropout': [0.2, 0.3, 0.4, 0.5]\n}\n\nscan_results = ta.Scan(X_train, np.array(y_train), params=para, model=imdb_fn)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d9c4858261edffb7b13ae1c2fc22953d9cbbbb2"
      },
      "cell_type": "markdown",
      "source": "Here is the link for more information about Talos implementation on GPU/TPU:\n\nhttps://github.com/rahulvs94/CNN-2D-on-CIFAR-10-dataset/blob/master/CIFAR10_Hyper_parameter_optimization_using_Talos_CPU_TPU.ipynb"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a89bae8e3259ddb92cce67eaa7bfb821b569e56e"
      },
      "cell_type": "markdown",
      "source": "Note:\n1. Recommend using TPU/GPU for faster optimization\n2. For understanding, work with small set of parameters"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9df1beab18af30c3a4df58e390ebd812d695abff"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}